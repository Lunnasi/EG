git clone https://github.com/Lunnasi/EG.git --branch EG/home_work

Multi-node cluster install

1. Download and intall JAva and Hadoop on all the systems
2. Specify the IP address of each system followed by their host names in host file of each system
3. COnfigure hadoop configuration files (core-site,hdfs-site,mapred-site,yasrn-site)
4. Edit slaves file on master node
5. Format namenode and start all hadoop services
6. Check live nodes on Hadoop namenode UI
   hdfs dfsadmin report 

Deploy server:
EGHome  -  192.168.56.105

Master Deamons will run on
master  -  192.168.56.11
HDFS - Name node
YARN - Resource Manager

Slave Deamons will run on
node1  -  192.168.56.12
node2  -  192.168.56.13
HDFS - Data node
YARN - Node manager

host_file:
192.168.56.11 master master
192.168.56.12 node1 node1
192.168.56.13 node2 node2
192.168.56.14 clickhouse clickhouse
192.168.56.105 eghome eghome


__________________________________
Theoretical part

Get as much knowledge as possible about modern state of Hadoop ecosystem during 2 week time period

Practical part 

Setup minimal Hadoop infrastructure on local VMs (VirtualBox or any other) using Ansible

Setup ClickHouse on local VM using Ansible

Create MR job (no restrictions on language/tooling) to transform CSV file with 3 fields, 1000000 rows from HDFS into ClickHouse table

 
Outcomes

Ansible code (in form of Git repository snapshot) to setup Hadoop and Clickhouse
Sources (in form of Git repository snapshot) to compile/submit MR job
Ability to demonstrate understanding of Hadoop ecosystem

__________________________________
Deadline for submitting practical part: 21.03.2019. In case you have any questions feel free to contact XXXXXX

